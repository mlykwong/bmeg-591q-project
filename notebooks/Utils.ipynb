{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2823f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3fd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register():\n",
    "    output_dir = \"../HaN-Seg Registered\"\n",
    "\n",
    "    for i in range(1, 43):\n",
    "        case_num = f\"{i:02d}\"\n",
    "        filepath_ct = f\"../HaN-Seg/set_1/case_{case_num}/case_{case_num}_IMG_CT.nrrd\"\n",
    "        filepath_mri = f\"../HaN-Seg/set_1/case_{case_num}/case_{case_num}_IMG_MR_T1.nrrd\"\n",
    "        ct = sitk.ReadImage(filepath_ct, sitk.sitkFloat32)\n",
    "        mri = sitk.ReadImage(filepath_mri, sitk.sitkFloat32)\n",
    "        \n",
    "        # Registers the MRI image to the CT image\n",
    "        mri_registered = register_helper(ct, mri)\n",
    "\n",
    "        # Saves image to the HaN-Seg Registration Folder\n",
    "        output_path = os.path.join(output_dir, f\"MRI_Case_{case_num}.nrrd\")\n",
    "        sitk.WriteImage(mri_registered, output_path)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Add notation here\n",
    "def register_helper(fixed_image, moving_image):\n",
    "\n",
    "    initial_transform = sitk.CenteredTransformInitializer(\n",
    "        fixed_image,\n",
    "        moving_image,\n",
    "        sitk.Euler3DTransform(),\n",
    "        sitk.CenteredTransformInitializerFilter.GEOMETRY,\n",
    "    )\n",
    "\n",
    "    min_value = float(sitk.GetArrayViewFromImage(moving_image).min())\n",
    "    moving_resampled = sitk.Resample(\n",
    "        moving_image,\n",
    "        fixed_image,\n",
    "        initial_transform,\n",
    "        sitk.sitkLinear,\n",
    "        min_value,\n",
    "        moving_image.GetPixelID(),\n",
    "    )\n",
    "\n",
    "    registration_method = sitk.ImageRegistrationMethod()\n",
    "\n",
    "    # Similarity metric settings.\n",
    "    registration_method.SetMetricAsMattesMutualInformation(numberOfHistogramBins=50)\n",
    "    registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)\n",
    "    registration_method.SetMetricSamplingPercentage(0.01)\n",
    "\n",
    "    registration_method.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "    # Optimizer settings.\n",
    "    registration_method.SetOptimizerAsGradientDescent(\n",
    "        learningRate=1.0,\n",
    "        numberOfIterations=100,\n",
    "        convergenceMinimumValue=1e-6,\n",
    "        convergenceWindowSize=10,\n",
    "    )\n",
    "    registration_method.SetOptimizerScalesFromPhysicalShift()\n",
    "\n",
    "    # Setup for the multi-resolution framework.\n",
    "    registration_method.SetShrinkFactorsPerLevel(shrinkFactors=[4, 2, 1])\n",
    "    registration_method.SetSmoothingSigmasPerLevel(smoothingSigmas=[2, 1, 0])\n",
    "    registration_method.SmoothingSigmasAreSpecifiedInPhysicalUnitsOn()\n",
    "\n",
    "    # Don't optimize in-place, we would possibly like to run this cell multiple times.\n",
    "    registration_method.SetInitialTransform(initial_transform, inPlace=False)\n",
    "\n",
    "    final_transform = registration_method.Execute(\n",
    "        sitk.Cast(fixed_image, sitk.sitkFloat32), sitk.Cast(moving_image, sitk.sitkFloat32)\n",
    "    )\n",
    "\n",
    "    moving_resampled = sitk.Resample(\n",
    "        moving_image,\n",
    "        fixed_image,\n",
    "        final_transform,\n",
    "        sitk.sitkLinear,\n",
    "        min_value,\n",
    "        moving_image.GetPixelID(),\n",
    "    )\n",
    "    return moving_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b4d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd7f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneVolume():\n",
    "    output_dir = \"../HaN-Seg Pruned\"\n",
    "    for i in range(1, 43):\n",
    "        case_num = f\"{i:02d}\"\n",
    "        filepath_mri = f\"../HaN-Seg Registered/MRI_Case_{case_num}.nrrd\"\n",
    "        mri = sitk.ReadImage(filepath_mri, sitk.sitkFloat32)\n",
    "        mri = sitk.GetArrayFromImage(mri).astype(np.float32)\n",
    "\n",
    "        filepath_ct = f\"../HaN-Seg/set_1/case_{case_num}/case_{case_num}_IMG_CT.nrrd\"\n",
    "        ct = sitk.ReadImage(filepath_ct, sitk.sitkFloat32)\n",
    "        ct = sitk.GetArrayFromImage(ct).astype(np.float32)\n",
    "\n",
    "        filepath = f\"../HaN-Seg/set_1/case_{case_num}/case_{case_num}_OAR_Bone_Mandible.seg.nrrd\"\n",
    "        gt = sitk.ReadImage(filepath, sitk.sitkFloat32)\n",
    "        gt = sitk.GetArrayFromImage(gt).astype(np.float32)\n",
    "\n",
    "        gt_var = np.var(gt, axis = (1, 2))\n",
    "        # mask = gt_var > 1e-10\n",
    "        mask = gt_var > 0.0001\n",
    "        mri = mri[mask]\n",
    "        mri = sitk.GetImageFromArray(mri)\n",
    "        ct = ct[mask]\n",
    "        ct = sitk.GetImageFromArray(ct)\n",
    "        gt = gt[mask]\n",
    "        gt = sitk.GetImageFromArray(gt)\n",
    "\n",
    "        output_path = os.path.join(output_dir, \"MRI\", f\"MRI_Case_{case_num}.nrrd\")\n",
    "        sitk.WriteImage(mri, output_path)\n",
    "\n",
    "        output_path = os.path.join(output_dir, \"CT\", f\"CT_Case_{case_num}.nrrd\")\n",
    "        sitk.WriteImage(ct, output_path)\n",
    "\n",
    "        output_path = os.path.join(output_dir, \"GT\", f\"GT_Case_{case_num}.nrrd\")\n",
    "        sitk.WriteImage(gt, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00b390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruneVolume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1018566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000\n",
    "RANDOM_SEED = 42\n",
    "IMAGE_RESIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc0a7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img_path, mean, std, output):\n",
    "    for i in range(len(img_path)):\n",
    "        img = sitk.ReadImage(img_path[i], sitk.sitkFloat32)\n",
    "        img = sitk.GetArrayFromImage(img).astype(np.float32)\n",
    "\n",
    "        normalized = (img - mean) / std\n",
    "        normalized = sitk.GetImageFromArray(normalized)\n",
    "\n",
    "        output_path = output + f\"{i}.nrrd\"\n",
    "        sitk.WriteImage(normalized, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a34f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNormal(img_path):\n",
    "    img_sum = 0.0\n",
    "    img_sq_sum = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for path in img_path:\n",
    "        img = sitk.ReadImage(path, sitk.sitkFloat32)\n",
    "        img = sitk.GetArrayFromImage(img).astype(np.float32)\n",
    "        img_sum += np.sum(img)\n",
    "        img_sq_sum += np.sum(img ** 2)\n",
    "        total += img.size\n",
    "\n",
    "    train_mean = img_sum/total\n",
    "    train_std = np.sqrt(img_sq_sum/total - (train_mean**2))\n",
    "\n",
    "    return train_mean, train_std       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(mri_path, ct_path, mask_path):\n",
    "    mri_path = mri_path.numpy().decode(\"utf-8\")\n",
    "    ct_path = ct_path.numpy().decode(\"utf-8\")\n",
    "    mask_path = mask_path.numpy().decode(\"utf-8\")\n",
    "\n",
    "    mri = sitk.ReadImage(mri_path, sitk.sitkFloat32)\n",
    "    ct = sitk.ReadImage(ct_path, sitk.sitkFloat32)\n",
    "    gt = sitk.ReadImage(mask_path, sitk.sitkFloat32)\n",
    "\n",
    "    slices_mri = sitk.GetArrayFromImage(mri).astype(np.float32)\n",
    "    slices_ct = sitk.GetArrayFromImage(ct).astype(np.float32)\n",
    "    slices_gt = sitk.GetArrayFromImage(gt).astype(np.float32)\n",
    "\n",
    "    slices_gt = (slices_gt > 0.5).astype(np.float32)\n",
    "\n",
    "    slices_mri_resize = np.expand_dims(slices_mri, axis=-1)\n",
    "    slices_ct_resize = np.expand_dims(slices_ct, axis=-1)\n",
    "    slices_gt_resize = np.expand_dims(slices_gt, axis=-1)\n",
    "\n",
    "    slices_mri_resize = tf.image.resize(slices_mri_resize, [IMAGE_RESIZE, IMAGE_RESIZE], method=tf.image.ResizeMethod.BILINEAR)\n",
    "    slices_ct_resize = tf.image.resize(slices_ct_resize, [IMAGE_RESIZE, IMAGE_RESIZE], method=tf.image.ResizeMethod.BILINEAR)\n",
    "    slices_gt_resize = tf.image.resize(slices_gt_resize, [IMAGE_RESIZE, IMAGE_RESIZE], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    slices = np.concatenate([slices_mri_resize, slices_ct_resize], axis=-1)\n",
    "\n",
    "    return slices, slices_gt_resize\n",
    "\n",
    "def lammy_func(mri_path, ct_path, mask_path):\n",
    "    img, mask = tf.py_function(\n",
    "        func=load_images,\n",
    "        inp=[mri_path, ct_path, mask_path],\n",
    "        Tout=[tf.float32, tf.float32]\n",
    "    )\n",
    "    img.set_shape([None, IMAGE_RESIZE, IMAGE_RESIZE, 2])\n",
    "    mask.set_shape([None, IMAGE_RESIZE, IMAGE_RESIZE, 1])\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca6a1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_path = []\n",
    "ct_path = []\n",
    "gt_path = []\n",
    "\n",
    "# mri_train_mean, mri_train_sd, ct_train_mean, ct_train_sd = normalize()\n",
    "\n",
    "for i in range(1, 43):\n",
    "    case_num = f\"{i:02d}\"\n",
    "    filepath_mri = f\"../HaN-Seg Pruned/MRI/MRI_Case_{case_num}.nrrd\"\n",
    "    mri_path.append(filepath_mri)\n",
    "    filepath_ct = f\"../HaN-Seg Pruned/CT/CT_Case_{case_num}.nrrd\"\n",
    "    ct_path.append(filepath_ct)\n",
    "    filepath_gt = f\"../HaN-Seg Pruned/GT/GT_Case_{case_num}.nrrd\"\n",
    "    gt_path.append(filepath_gt)\n",
    "\n",
    "mri_train, mri_test, ct_train, ct_test, gt_train, gt_test = train_test_split(mri_path, ct_path, gt_path, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "mri_mean, mri_std = computeNormal(mri_train)\n",
    "ct_mean, ct_std = computeNormal(ct_train)\n",
    "\n",
    "normalize(mri_train, mri_mean, mri_std, \"../HaN-Seg Pruned/Train/MRI/\")\n",
    "normalize(ct_train, ct_mean, ct_std, \"../HaN-Seg Pruned/Train/CT/\")\n",
    "\n",
    "mri_norm_train = []\n",
    "ct_norm_train = []\n",
    "\n",
    "for i in range(len(mri_train)):\n",
    "    mri_norm_train.append(f\"../HaN-Seg Pruned/Train/MRI/{i}.nrrd\")\n",
    "    ct_norm_train.append(f\"../HaN-Seg Pruned/Train/CT/{i}.nrrd\")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((mri_norm_train, ct_norm_train, gt_train))\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset\n",
    "    .map(lammy_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .unbatch()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "normalize(mri_test, mri_mean, mri_std, \"../HaN-Seg Pruned/Test/MRI/\")\n",
    "normalize(ct_test, ct_mean, ct_std, \"../HaN-Seg Pruned/Test/CT/\")\n",
    "\n",
    "mri_norm_test = []\n",
    "ct_norm_test = []\n",
    "\n",
    "for i in range(len(mri_test)):\n",
    "    mri_norm_test.append(f\"../HaN-Seg Pruned/Test/MRI/{i}.nrrd\")\n",
    "    ct_norm_test.append(f\"../HaN-Seg Pruned/Test/CT/{i}.nrrd\")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mri_norm_test, ct_norm_test, gt_test))\n",
    "\n",
    "test_dataset = (\n",
    "    test_dataset\n",
    "    .map(lammy_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .unbatch()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9722ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A single Encoding step in the Contracting path of a U-Net CNN\n",
    "## @Inputs:\n",
    "##       inputs: image of size (nxn) with k feature channels\n",
    "##       num_channels: number of channels to have in output image (i.e. depth of output tensor)\n",
    "## @Outputs: \n",
    "##       x: image of size (n/2 x n/2) with num_channels feature channels\n",
    "def encode_block(inputs, num_channels):\n",
    "    # Extract num_channels feature channels from image\n",
    "    x = tf.keras.layers.Conv2D(num_channels, 3, padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(num_channels, 3, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    skip = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # Downsample each channels feature map by a factor of 2\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2)(skip)\n",
    "\n",
    "    return skip, x\n",
    "\n",
    "## A single Decoding step in the Expanding path of a U-Net CNN\n",
    "## @Inputs:\n",
    "##       inputs: image of size (nxn) with k feature channels\n",
    "##       skip_connection: tensor of corresponding encoding block\n",
    "##       num_channels: number of channels to have in output image (i.e. depth of output tensor)\n",
    "## @Outputs: \n",
    "##       x: image of size (2nx2n) with num_channels feature channels\n",
    "def decode_block(inputs, skip_connection, num_channels):\n",
    "    # Upsample image by doubling feature space while changing feature channels to num_channels\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_channels, (2,2), strides=2, padding='same')(inputs)\n",
    "\n",
    "    # Concatenate the skip_channel and the upsampled image (doubles the feature channels)\n",
    "    # Might need to resize skip_connection, but should be fine b/c same padding in encoding\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_connection])\n",
    "    \n",
    "    # Merge feature channels from the skip_connection and upsampled input image\n",
    "    x = tf.keras.layers.Conv2D(num_channels, 3, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(num_channels, 3, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Metrics\n",
    "# Source: https://medium.com/mastering-data-science/understanding-evaluation-metrics-in-medical-image-segmentation-d289a373a3f\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    epsilon = 1e-6\n",
    "    threshold = 0.5\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    # y_pred = tf.cast(y_pred, tf.float32)\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))\n",
    "    return (2*tp+epsilon)/(2*tp + fp + fn+epsilon)\n",
    "\n",
    "def rand_index(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))\n",
    "    return tp / (tp + fp + fn)\n",
    "\n",
    "def jaccard_index(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))\n",
    "    tn = tf.reduce_sum(tf.cast((1 - y_true) * (1 - y_pred), tf.float32))\n",
    "    return (tp + tn) / (tp + tn + fn + fp)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    true_negatives = tf.reduce_sum(tf.cast((1 - y_true) * (1 - y_pred), tf.float32))\n",
    "    possible_negatives = tf.reduce_sum(tf.cast(1 - y_true, tf.float32))\n",
    "    return true_negatives / (possible_negatives + tf.keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5077756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss1(y_true, y_pred):\n",
    "    dice_weight = 50.0\n",
    "    bce_weight = 1.0\n",
    "    bce = tf.keras.losses.BinaryFocalCrossentropy(alpha=0.75)\n",
    "    loss_dice = 1.0 - dice_coeff(y_true, y_pred)\n",
    "    loss_bce = bce(y_true, y_pred)\n",
    "    loss = dice_weight * loss_dice + bce_weight * loss_bce\n",
    "    return loss\n",
    "\n",
    "def custom_loss2(y_true, y_pred):\n",
    "    dice_weight = 50.0\n",
    "    bce_weight = 1.0\n",
    "    bce = tf.keras.losses.BinaryFocalCrossentropy(alpha=0.25)\n",
    "    loss_dice = 1.0 - dice_coeff(y_true, y_pred)\n",
    "    loss_bce = bce(y_true, y_pred)\n",
    "    loss = dice_weight * loss_dice + bce_weight * loss_bce\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2da3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the model\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(IMAGE_RESIZE, IMAGE_RESIZE, 2))\n",
    "\n",
    "# Do 5 calls of encode_block to end up with a 32x32x512 tensor\n",
    "s1, e1 = encode_block(input, 32)\n",
    "s2, e2 = encode_block(e1, 64)\n",
    "s3, e3 = encode_block(e2, 128)\n",
    "s4, e4 = encode_block(e3, 256)\n",
    "# s5, e5 = encode_block(e4, 512)\n",
    "\n",
    "# Bottleneck\n",
    "# b1 = tf.keras.layers.Conv2D(1024, 3, padding='same')(e5)\n",
    "# b1 = tf.keras.layers.Activation('relu')(b1)\n",
    "# b1 = tf.keras.layers.Conv2D(1024, 3, padding='same')(b1)\n",
    "# b1 = tf.keras.layers.Activation('relu')(b1)\n",
    "\n",
    "b1 = tf.keras.layers.Conv2D(512, 3, padding='same')(e4)\n",
    "b1 = tf.keras.layers.Activation('relu')(b1)\n",
    "b1 = tf.keras.layers.Conv2D(512, 3, padding='same')(b1)\n",
    "b1 = tf.keras.layers.Activation('relu')(b1)\n",
    "\n",
    "# Do 5 calls of decode_block\n",
    "# d1 = decode_block(b1, s5, 512)\n",
    "d2 = decode_block(b1, s4, 256)\n",
    "d3 = decode_block(d2, s3, 128)\n",
    "d4 = decode_block(d3, s2, 64)\n",
    "d5 = decode_block(d4, s1, 32)\n",
    "\n",
    "# Play around with activation\n",
    "output = tf.keras.layers.Conv2D(1, 1, padding='same', activation='sigmoid')(d5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "     75/Unknown \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.9871 - dice_coeff: 0.1926 - jaccard_index: 0.6633 - loss: 40.4197 - rand_index: 0.0031 - specificity: 0.6636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlykw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 2s/step - accuracy: 0.9918 - dice_coeff: 0.2489 - jaccard_index: 0.6783 - loss: 37.3363 - rand_index: 0.0032 - specificity: 0.6787 - val_accuracy: 0.9843 - val_dice_coeff: 0.1111 - val_jaccard_index: 0.5677 - val_loss: 44.0321 - val_rand_index: 0.0028 - val_specificity: 0.5680 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - accuracy: 0.9976 - dice_coeff: 0.3318 - jaccard_index: 0.7514 - loss: 33.0875 - rand_index: 0.0038 - specificity: 0.7520 - val_accuracy: 0.9860 - val_dice_coeff: 0.1760 - val_jaccard_index: 0.6367 - val_loss: 40.4123 - val_rand_index: 0.0041 - val_specificity: 0.6369 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9980 - dice_coeff: 0.3527 - jaccard_index: 0.7815 - loss: 32.0164 - rand_index: 0.0043 - specificity: 0.7822 - val_accuracy: 0.9937 - val_dice_coeff: 0.1751 - val_jaccard_index: 0.6907 - val_loss: 40.4334 - val_rand_index: 0.0037 - val_specificity: 0.6914 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9983 - dice_coeff: 0.4238 - jaccard_index: 0.7976 - loss: 28.3877 - rand_index: 0.0050 - specificity: 0.7983 - val_accuracy: 0.9957 - val_dice_coeff: 0.2906 - val_jaccard_index: 0.7327 - val_loss: 34.0911 - val_rand_index: 0.0049 - val_specificity: 0.7333 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9986 - dice_coeff: 0.5425 - jaccard_index: 0.8103 - loss: 22.3508 - rand_index: 0.0057 - specificity: 0.8109 - val_accuracy: 0.9974 - val_dice_coeff: 0.2856 - val_jaccard_index: 0.7619 - val_loss: 34.3614 - val_rand_index: 0.0051 - val_specificity: 0.7628 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 2s/step - accuracy: 0.9988 - dice_coeff: 0.6047 - jaccard_index: 0.8200 - loss: 19.1864 - rand_index: 0.0062 - specificity: 0.8206 - val_accuracy: 0.9979 - val_dice_coeff: 0.2855 - val_jaccard_index: 0.7781 - val_loss: 34.3640 - val_rand_index: 0.0054 - val_specificity: 0.7790 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9989 - dice_coeff: 0.6455 - jaccard_index: 0.8279 - loss: 17.1038 - rand_index: 0.0067 - specificity: 0.8285 - val_accuracy: 0.9980 - val_dice_coeff: 0.2368 - val_jaccard_index: 0.7877 - val_loss: 37.0332 - val_rand_index: 0.0052 - val_specificity: 0.7886 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9991 - dice_coeff: 0.6921 - jaccard_index: 0.8345 - loss: 14.7381 - rand_index: 0.0072 - specificity: 0.8351 - val_accuracy: 0.9980 - val_dice_coeff: 0.3580 - val_jaccard_index: 0.7964 - val_loss: 30.3925 - val_rand_index: 0.0065 - val_specificity: 0.7973 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9992 - dice_coeff: 0.7284 - jaccard_index: 0.8401 - loss: 12.8972 - rand_index: 0.0077 - specificity: 0.8406 - val_accuracy: 0.9982 - val_dice_coeff: 0.3878 - val_jaccard_index: 0.8027 - val_loss: 28.7568 - val_rand_index: 0.0070 - val_specificity: 0.8035 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9993 - dice_coeff: 0.7610 - jaccard_index: 0.8453 - loss: 11.2496 - rand_index: 0.0083 - specificity: 0.8458 - val_accuracy: 0.9986 - val_dice_coeff: 0.4204 - val_jaccard_index: 0.8094 - val_loss: 26.9974 - val_rand_index: 0.0070 - val_specificity: 0.8103 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9993 - dice_coeff: 0.7819 - jaccard_index: 0.8502 - loss: 10.2068 - rand_index: 0.0088 - specificity: 0.8507 - val_accuracy: 0.9988 - val_dice_coeff: 0.5261 - val_jaccard_index: 0.8147 - val_loss: 21.2744 - val_rand_index: 0.0082 - val_specificity: 0.8154 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9994 - dice_coeff: 0.8011 - jaccard_index: 0.8546 - loss: 9.2607 - rand_index: 0.0092 - specificity: 0.8551 - val_accuracy: 0.9989 - val_dice_coeff: 0.5695 - val_jaccard_index: 0.8195 - val_loss: 19.1595 - val_rand_index: 0.0089 - val_specificity: 0.8202 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9994 - dice_coeff: 0.8172 - jaccard_index: 0.8586 - loss: 8.4750 - rand_index: 0.0096 - specificity: 0.8591 - val_accuracy: 0.9989 - val_dice_coeff: 0.5788 - val_jaccard_index: 0.8237 - val_loss: 18.7809 - val_rand_index: 0.0089 - val_specificity: 0.8245 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9995 - dice_coeff: 0.8230 - jaccard_index: 0.8623 - loss: 8.1795 - rand_index: 0.0100 - specificity: 0.8628 - val_accuracy: 0.9989 - val_dice_coeff: 0.6151 - val_jaccard_index: 0.8277 - val_loss: 16.9351 - val_rand_index: 0.0099 - val_specificity: 0.8283 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9995 - dice_coeff: 0.8330 - jaccard_index: 0.8659 - loss: 7.7098 - rand_index: 0.0105 - specificity: 0.8664 - val_accuracy: 0.9991 - val_dice_coeff: 0.6613 - val_jaccard_index: 0.8313 - val_loss: 14.8612 - val_rand_index: 0.0099 - val_specificity: 0.8320 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 2s/step - accuracy: 0.9995 - dice_coeff: 0.8403 - jaccard_index: 0.8692 - loss: 7.3504 - rand_index: 0.0109 - specificity: 0.8697 - val_accuracy: 0.9991 - val_dice_coeff: 0.6724 - val_jaccard_index: 0.8349 - val_loss: 14.2910 - val_rand_index: 0.0104 - val_specificity: 0.8356 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9995 - dice_coeff: 0.8275 - jaccard_index: 0.8721 - loss: 7.9570 - rand_index: 0.0112 - specificity: 0.8726 - val_accuracy: 0.9979 - val_dice_coeff: 0.2293 - val_jaccard_index: 0.8388 - val_loss: 37.4969 - val_rand_index: 0.0055 - val_specificity: 0.8401 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.9995 - dice_coeff: 0.8440 - jaccard_index: 0.8753 - loss: 7.1519 - rand_index: 0.0116 - specificity: 0.8758 - val_accuracy: 0.9986 - val_dice_coeff: 0.4848 - val_jaccard_index: 0.8410 - val_loss: 23.8541 - val_rand_index: 0.0085 - val_specificity: 0.8421 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 2s/step - accuracy: 0.9995 - dice_coeff: 0.8519 - jaccard_index: 0.8783 - loss: 6.7633 - rand_index: 0.0121 - specificity: 0.8787 - val_accuracy: 0.9991 - val_dice_coeff: 0.6768 - val_jaccard_index: 0.8432 - val_loss: 14.0390 - val_rand_index: 0.0112 - val_specificity: 0.8440 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8580 - jaccard_index: 0.8811 - loss: 6.4616 - rand_index: 0.0126 - specificity: 0.8815 - val_accuracy: 0.9992 - val_dice_coeff: 0.7043 - val_jaccard_index: 0.8475 - val_loss: 12.6895 - val_rand_index: 0.0121 - val_specificity: 0.8482 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 1/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.9995 - dice_coeff: 0.8355 - jaccard_index: 0.8866 - loss: 7.5605 - rand_index: 0.0131 - specificity: 0.8870 - val_accuracy: 0.9987 - val_dice_coeff: 0.5650 - val_jaccard_index: 0.8587 - val_loss: 19.9418 - val_rand_index: 0.0107 - val_specificity: 0.8597 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8641 - jaccard_index: 0.8956 - loss: 6.1623 - rand_index: 0.0149 - specificity: 0.8960 - val_accuracy: 0.9994 - val_dice_coeff: 0.7920 - val_jaccard_index: 0.8660 - val_loss: 7.9694 - val_rand_index: 0.0169 - val_specificity: 0.8665 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8679 - jaccard_index: 0.9030 - loss: 5.9603 - rand_index: 0.0165 - specificity: 0.9034 - val_accuracy: 0.9993 - val_dice_coeff: 0.7774 - val_jaccard_index: 0.8718 - val_loss: 8.7348 - val_rand_index: 0.0184 - val_specificity: 0.8723 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8569 - jaccard_index: 0.9088 - loss: 6.4474 - rand_index: 0.0177 - specificity: 0.9092 - val_accuracy: 0.9987 - val_dice_coeff: 0.4712 - val_jaccard_index: 0.8782 - val_loss: 24.2108 - val_rand_index: 0.0135 - val_specificity: 0.8793 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8611 - jaccard_index: 0.9136 - loss: 6.2962 - rand_index: 0.0187 - specificity: 0.9140 - val_accuracy: 0.9991 - val_dice_coeff: 0.6416 - val_jaccard_index: 0.8825 - val_loss: 15.3855 - val_rand_index: 0.0171 - val_specificity: 0.8834 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8646 - jaccard_index: 0.9179 - loss: 6.1284 - rand_index: 0.0199 - specificity: 0.9183 - val_accuracy: 0.9990 - val_dice_coeff: 0.6416 - val_jaccard_index: 0.8854 - val_loss: 15.7363 - val_rand_index: 0.0172 - val_specificity: 0.8863 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8926 - jaccard_index: 0.9335 - loss: 5.3666 - rand_index: 0.0220 - specificity: 0.9340\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 2s/step - accuracy: 0.9996 - dice_coeff: 0.8721 - jaccard_index: 0.9221 - loss: 5.6923 - rand_index: 0.0217 - specificity: 0.9225 - val_accuracy: 0.9991 - val_dice_coeff: 0.6720 - val_jaccard_index: 0.8886 - val_loss: 14.2726 - val_rand_index: 0.0192 - val_specificity: 0.8895 - learning_rate: 1.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 150\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "#######################\n",
    "# TODO BEFORE NEXT RUN:\n",
    "# RENAME THE CSV\n",
    "# SAVE MODELS\n",
    "#######################\n",
    "\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_dice_coeff',\n",
    "    factor = 0.5,\n",
    "    patience = 5,\n",
    "    mode='max',\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_dice_coeff',\n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model1 = tf.keras.models.Model(inputs=input, outputs=output, name='U-Net')\n",
    "\n",
    "model1.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=custom_loss1,\n",
    "    metrics=['accuracy', dice_coeff, specificity, rand_index, jaccard_index])\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('model1_150epoch.csv', append=False)\n",
    "\n",
    "history1 = model1.fit(\n",
    "    train_dataset, \n",
    "    validation_data=test_dataset,\n",
    "    epochs = EPOCH,\n",
    "    callbacks=[csv_logger, lr_reducer, early_stop]\n",
    "    )\n",
    "\n",
    "model2 = tf.keras.models.Model(inputs=input, outputs=output, name='U-Net')\n",
    "\n",
    "model2.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=custom_loss2,\n",
    "    metrics=['accuracy', dice_coeff, specificity, rand_index, jaccard_index])\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('model2_150epoch.csv', append=False)\n",
    "\n",
    "history2 = model2.fit(\n",
    "    train_dataset, \n",
    "    validation_data=test_dataset,\n",
    "    epochs = EPOCH,\n",
    "    callbacks=[csv_logger, lr_reducer, early_stop]\n",
    "    )\n",
    "\n",
    "model1.save(\"model1_good_150epoch.keras\")\n",
    "model2.save(\"model2_good_150epoch.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57351a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"model1_good_150epoch.keras\")\n",
    "model2.save(\"model2_good_150epoch.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
